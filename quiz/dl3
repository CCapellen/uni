Q: What are some typical activation functions? What properties do they have?
A:
Q: What is a feed forward network?
A:
Q: What is slide 8, output units telling me?
A:
Q: What is the difference between output units and hidden units and what activation functions are commonly used for them?
A: 
Q: Why is it important to use activations which are non-linear?
A: succesion of nonlinear activations collapses to one activation
Q: What is a Sparse Network? 
A:
Q: What is the connection between using sigmoidal activation functions and skip layer connections?
A: sigmoid function is almost linear if the weights are small, then layers can 'collaps'
Q: What is a feed-forward architecture?
A:
Q: What is the Weight-Space Symmetry?
A:
Q: Why might learning fail?
A:
Q: What is a possible error function?
A:
Q: What is the idea of Gradient Descent Optimization?
A:
Q: How should you initialize the weights? why?
A:
Q: How many hidden units should you use for a problem? what influence does their number have?
A: Too many - easy to overfit; not enough - insufficient representation power. Having too many hidden units is generally better than not having enough. 
Q: Question on slide 40
A:
Q: What are McCulloch-Pitts?
A: A computational model of a neuron: It is binary and has excitatory and inhibitory inputs. The output is a thresholded sum, if none of the inhibitory inputs is 1.
Q: build a OR and AND function using a McCulloch Pitts unit 
A:
Q: What is a perceptron?
A: weighted input gets thresholded; apparently can also describe network. Historically it was only a single layer network but today it might mean any standard feed-forward network
Q: Describe the capability of single perceptrons
A: one or a layer of one? Is a linear mapping, equivalent to finding a line, cannot model XOR



