Q: What is computational learning theory (COLT)?
A: different formal mathematical models of learning that enable rigorous
analysis of learning algorithms
Q: What parts does a learning model have? Describe each part
A: learner: Who is doing the learning? domain: What is being learned? information source: From what is the learner learning? prior knowledge: What does the learner know about the domain initially? performance criteria: What is the learner‘s output? How do we know whether, or how well, the learner has learned?
Q: What is the mistake bound model for conjunctive concepts?
A: performance is measured through prediction mistakes, the source is noise-free, c is a halfspace c={x /in R^n: w*x /geq 0}, c:X -> {-1,1}. Mistake-bound model of learning: puts an upper bound on the total number of mistakes
that the learner ever will make
Q: State the Algorithm FIND_Sconjunctions. 
A: slide 10
Q: What is the upper bound for the mistake of FIND_Sconjunctions? + sketch proof
A: n+1
Q: What is the mistake bound model for learning halfspaces?
A: /leq (R/y)^2
Q: State the perceptron algorithm. What is the upper mistake? + sketch proof
A: slide 14
Q: What is the PAC Model of Learning? What is the definition of a PAC-learnable concept class?
A: probability distribution D is completly arbitrary, stationary and unknown. Random oracle returns labeled examples randomly and idependently chosen according to x. Goal is to indentify c. 
Q: Definition of PAC-learning
A: A concept class C is PAC-learnable if there exists an algorithm L with the following property: for every concept c /in C for every distribution D on X for every 0 < /epsilon < 1 for every 0 < /delta < 1 is the error(h) < /epsilon
Q: Definition of efficient PAC-learning
A: C is efficiently PAC-learnable and there exists a PAC-learning algorithm for C that runs in time polynomial in 1/ /epsilon, 1/ /delta, n, and size(c)
Q: State PAC-learning of conjunctive concepts. Is FIND_Sconjunctions a valid algorithm for the PAC model. Prove! Is conjunctions efficiently PAC-learnable?
A: yes
Q: What is a finite concept class and a finite hypothesis class? Are they PAC-learnable? Prove!
A: yes, if we have enough examples
Q: Are infinite concept classes PAC-learnable?
A: if the VCdim(H) is finite
Q: What is the Vapnik-Chervonenkis Dimension?
A: Measure for the capacity of a space of functions that can be learned by a statistical classification algorithm (Wiki).
Q: What was Erdös Question?
A:
Q: Def shattered, VCdim
A:
