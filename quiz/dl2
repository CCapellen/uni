Q: State Bayes Rule. What do the parts mean?
A: P(y|x) = \frac{Pr(x|y)*Pr(y)}{\int Pr(x|y)*Pr(y) dy}; Posterior = Likelihood*Prior/Evidence; Evidence for normalization
Q: What is a learning Algorithm?
A: "A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E." (Mitchell, 1997)
Q: Describe the Machine Learning Tasks: classification, regression, structured prediction and density estimation
A: classification: f: R^n -> {1,...k}; regression: f:R^n -> R^m; structured prediction: vector output with well-defined relationships between the elements?; density estimation: learn probability mass function p_model: R^n -> R^m
Q: What is supervised/ unsupervised learning and how would you express the task in a probabilistic sense?
A: In supervised learning there is a label y given for each data point x. You estimate P(y|x). Unsupervised learning has no labels. You estimate the data generating distribution p_data(x)?
Q: what is under-/ overfitting?
A: underfitting: Algorithm has high training error \n overfitting: Algorithm has low training error but a high test error
Q: What is the model capacity?
A: the capability of the algorithm to reach the correct answer?
Q: when is the model capacity of a learning Algorithm optimal?
A: When the capacity equals the complexity of the target. If the capacity is too low, then the model will underfit. If it is too high it will overfit. STILL BETTER TOO HIGH?
Q: How do you calculate the L1 distance of two images? (How the L2)
A: subtracting pixel wise and then adding all differences.
Q: How works nearest neighbor estimation
A: You check to which training instance your test instance is closest and then assign the same label
Q: How should you choose Hyperparameters?
A: Cross-validate. Do not use the test data.
Q: What is a point estimator? 
A: A function of the data which estimates a single value which serves as best estimate for an unknown parameter or function. (It only gives one point estimation in contrast to a possible interval, it does not have to be a good estimate, but it only gives one estimate)
Q: What is function approximation?
A: book: estimates the relationship between input and target variables. lecture: Approximates the true f with a f* based on some model, which is usually predetermined, so it can be simplified to point estimation of the function parameter.
Q: How is the bias defined? When is the estimator unbiased, when asymptotically unbiased?
A: Bias(Estimator) = E(Estimator) - true value; The estimator is unbiased if the difference between the expectation and true value is zero. It is asymptotically unbiased if the bias is zero only if m->inf ???
Q: How is the variance defined? How the standard error?
A: Var(Est_m) = E[(Est - E[Est])^2] = E[Est^2] - (E[Est])^2. The standart error is the square root of the variance.
Q: How is the mean squared error defined?
A: MSE(Est_m)  = E[(Est-true)^2] = Bias(Est_m)^2 + Var(Est_m)
Q: How works maximum likelihood estimation? When does it converge to the true value?
A: You look for the estimator which maximizes the joint probability of the data and the estimator. Est_ML = argmax_{Est} p_model(data,Est) = argmax_{Est} \product_{i=1}^{m} p_model(data_i, Est) = argmax_{est} \sum_{i=1}^m log p_model(x_i,est). Similarily for p(Y|data,Est): Est_ML = argmin_{Est} -\sum_{i=1}^m log p(y_i|data_i,Est). It converges to the true value if pdata lies in the model family, and pdata corresponds to exactly one value of est.
Q: What is the MAP Estimation?
A: It chooses the estimator which maximizes the probability of the estimator given the data. This can be formed to the following equation: Est_MAP = argmax_{Est} [\sum_{i=1}^m log p(x_i|Est) + log p(Est)] and for the conditional likelihood: Est_MAP = argmax_{Est} [\sum_{i=1}^m log P(y_i|x_i,Est) + log p(Est)]
Q: What is the relation between model capacity, bias and variance?
A: increasing capacity tends to increase variance and decrease bias
