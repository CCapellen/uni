Q: Define Eulers Formula
A: v - e + f = 2
Q: How is a metric defined?
A: a metric satisfies the following: 1) Distance is always positive, and distance between two points is 0 iff points are the same. 2) Distance is symmetric 3) triangle inequation
Q: Describe K-nearest neighbors. Which parameters does it have?
A: K nearest neighbor calculates for a test example the distances to the training examples and lets the k nearest neighbors vote for the class prediction of the test example according to their own classification. It does lazy learning because it does not produce an explicit generalization. Parameters are k, the distance metric
Q: Which influence has the value of k on KNN? How can you determine the optimal one?
A: small k -> risk of overfitting, large k -> risk of underfitting. Determining k: cross-validation or can be done in a
single run through the database: Find neighbor n 1 , see if prediction is wrong, update error count for k=1 accordingly,Find subsequent neighbors and update corresponding error counts
Q: What is Out-of-Sample Testing?
A: unbiased estimator of true error.Split available data 70/30 into training and test set
Q: Give pseudo code for n-fold cross validation
A: slide 26
Q: Why is scaling of the attribute values important?
A: the attributes with smaller absolute distances will be effectively “ignored”
Q: Why is weightening of attributes a good idea?
A: learning algorithm might become confused by large numbers of (potentially irrelevant) attributes (for example KNN)
Q: What is a problem with distance in high-dimensional spaces?
A: They  get more and more similar (slide 29)
Q: Which properties (good and bad ones) has KNN?
A: slide 30
